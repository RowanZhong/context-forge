"""
åœºæ™¯ 2ï¼šå¤šè½®å¯¹è¯è®°å¿†ç®¡ç†

æ¼”ç¤ºå¦‚ä½•åœ¨é•¿å¯¹è¯ä¸­ç®¡ç†ä¸Šä¸‹æ–‡ï¼š
- 20 è½®å¯¹è¯å†å²
- must_keep å…³é”®ä¿¡æ¯ä¿æŠ¤
- æ»‘åŠ¨çª—å£ç­–ç•¥
- å†å²å‹ç¼©
- æ—¶æ•ˆæ€§è¡°å‡

ä½¿ç”¨æ–¹æ³•ï¼š
  python examples/scenario_conversation_memory.py          # ä½¿ç”¨ mock æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
  python examples/scenario_conversation_memory.py --mock   # æ˜ç¡®æŒ‡å®š mock
  python examples/scenario_conversation_memory.py --no-mock # ä½¿ç”¨çœŸå® LLMï¼ˆéœ€è¦ API Keyï¼‰

â†’ 6.2.4 å‹ç¼©ç­–ç•¥
â†’ 6.3.3 Compressï¼šå‹ç¼©ç­–ç•¥å¼•æ“
"""

import asyncio
import sys
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from examples._shared import (
    MockLLM,
    check_api_key,
    console,
    create_comparison_table,
    format_percentage,
    format_tokens,
    parse_args,
    print_header,
    print_section,
    print_success,
    print_warning,
    truncate_text,
)


async def main(mock: bool = True):
    """ä¸»å‡½æ•°ã€‚"""
    from context_forge import ContextForge
    from context_forge.models.control import ControlFlags
    from context_forge.models.segment import Priority, Segment, SegmentType
    from context_forge.models.provenance import Provenance, SourceType
    from context_forge.models.metadata import SegmentMetadata

    print_header(
        "åœºæ™¯ 2ï¼šå¤šè½®å¯¹è¯è®°å¿†ç®¡ç†",
        "æ¼”ç¤ºå¦‚ä½•åœ¨é•¿å¯¹è¯ä¸­å‹ç¼©å†å²ã€ä¿æŠ¤å…³é”®ä¿¡æ¯ã€ä¼˜åŒ– Token ä½¿ç”¨"
    )

    # æ£€æŸ¥ API Key
    if not check_api_key(mock):
        mock = True

    print_section("æ­¥éª¤ 1ï¼šæ„å»º 20 è½®å¯¹è¯å†å²")

    # åˆ›å»ºå¯¹è¯å†å²ï¼ˆæ¨¡æ‹Ÿæ—…è¡Œè§„åˆ’åŠ©æ‰‹ï¼‰
    now = datetime.now()
    conversation_history = [
        # ç¬¬ 1 è½®ï¼ˆ15 å¤©å‰ï¼‰- åˆæ¬¡å’¨è¯¢
        {"role": "user", "content": "æˆ‘æƒ³è®¡åˆ’ä¸€æ¬¡æ—¥æœ¬æ—…è¡Œï¼Œæ—¶é—´åœ¨ 5 æœˆä»½ï¼Œé¢„ç®— 2 ä¸‡å…ƒã€‚", "turn": 1, "days_ago": 15, "must_keep": True},
        {"role": "assistant", "content": "å¥½çš„ï¼5 æœˆä»½å»æ—¥æœ¬æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ã€‚è¯·é—®æ‚¨è®¡åˆ’å»å‡ å¤©ï¼Ÿä¸»è¦æƒ³å»å“ªäº›åŸå¸‚ï¼Ÿ", "turn": 1, "days_ago": 15},

        # ç¬¬ 2 è½®ï¼ˆ14 å¤©å‰ï¼‰
        {"role": "user", "content": "å¤§æ¦‚ 7-10 å¤©ï¼Œæƒ³å»ä¸œäº¬å’Œäº¬éƒ½ã€‚", "turn": 2, "days_ago": 14, "must_keep": True},
        {"role": "assistant", "content": "7-10 å¤©æ¸¸ä¸œäº¬å’Œäº¬éƒ½å¾ˆå……è£•ã€‚2 ä¸‡å…ƒé¢„ç®—å¯ä»¥ç©å¾—ä¸é”™ã€‚å»ºè®®ä¸œäº¬ 4 å¤©ï¼Œäº¬éƒ½ 3 å¤©ï¼Œå¯ä»¥å®‰æ’ 1-2 å¤©å¤§é˜ªã€‚", "turn": 2, "days_ago": 14},

        # ç¬¬ 3-5 è½®ï¼ˆ13-11 å¤©å‰ï¼‰- é—²èŠ
        {"role": "user", "content": "æ—¥æœ¬çš„æ¨±èŠ±å­£æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ", "turn": 3, "days_ago": 13},
        {"role": "assistant", "content": "3 æœˆä¸‹æ—¬åˆ° 4 æœˆä¸Šæ—¬æ˜¯å…³ä¸œåœ°åŒºçš„æ¨±èŠ±å­£ï¼Œ5 æœˆå·²è¿‡èŠ±æœŸï¼Œä½†å¯ä»¥çœ‹åˆ°æ–°ç»¿å’Œç´«è—¤ã€‚", "turn": 3, "days_ago": 13},
        {"role": "user", "content": "é‚£è¾¹å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", "turn": 4, "days_ago": 12},
        {"role": "assistant", "content": "5 æœˆæ—¥æœ¬æ°”æ¸©åœ¨ 15-25â„ƒï¼Œæ¸©å’Œèˆ’é€‚ï¼Œå¶å°”æœ‰é›¨ã€‚å»ºè®®å¸¦è–„å¤–å¥—å’Œé›¨å…·ã€‚", "turn": 4, "days_ago": 12},
        {"role": "user", "content": "éœ€è¦åŠç­¾è¯å—ï¼Ÿ", "turn": 5, "days_ago": 11},
        {"role": "assistant", "content": "æ˜¯çš„ï¼Œä¸­å›½å…¬æ°‘å‰å¾€æ—¥æœ¬éœ€è¦ç­¾è¯ã€‚å»ºè®®æå‰ 1 ä¸ªæœˆåŠç†ï¼Œå‡†å¤‡æŠ¤ç…§ã€ç…§ç‰‡ã€åœ¨èŒè¯æ˜ç­‰ææ–™ã€‚", "turn": 5, "days_ago": 11},

        # ç¬¬ 6-8 è½®ï¼ˆ10-8 å¤©å‰ï¼‰- æ·±å…¥è§„åˆ’
        {"role": "user", "content": "æˆ‘å¯¹åŠ¨æ¼«å’Œç”µå­äº§å“æ¯”è¾ƒæ„Ÿå…´è¶£ã€‚", "turn": 6, "days_ago": 10, "must_keep": True},
        {"role": "assistant", "content": "é‚£ç§‹å¶åŸå’Œä¸­é‡ç™¾è€æ±‡ä¸€å®šè¦å»ï¼ç§‹å¶åŸæ˜¯ç”µå­äº§å“å’ŒåŠ¨æ¼«å‘¨è¾¹çš„å¤©å ‚ï¼Œä¸­é‡æœ‰å¾ˆå¤šå¤å¤ç©å…·åº—ã€‚", "turn": 6, "days_ago": 10},
        {"role": "user", "content": "æœ‰æ¨èçš„ä½å®¿å—ï¼Ÿ", "turn": 7, "days_ago": 9},
        {"role": "assistant", "content": "ä¸œäº¬å¯ä»¥ä½æ–°å®¿æˆ–æµ…è‰ï¼Œäº¤é€šæ–¹ä¾¿ã€‚äº¬éƒ½æ¨èä½æ²³åŸç”ºæˆ–äº¬éƒ½ç«™é™„è¿‘ã€‚é¢„ç®—æ¯æ™š 500-800 å…ƒçš„è¯å¯ä»¥ä½ä¸é”™çš„å•†åŠ¡é…’åº—ã€‚", "turn": 7, "days_ago": 9},
        {"role": "user", "content": "ç¾é£Ÿæ–¹é¢æœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ", "turn": 8, "days_ago": 8},
        {"role": "assistant", "content": "ä¸œäº¬å¿…åƒï¼šç­‘åœ°å¸‚åœºå¯¿å¸ã€ä¸€å…°æ‹‰é¢ã€çŸ¶ä¸¸æ°´äº§ã€‚äº¬éƒ½æ¨èï¼šé”¦å¸‚åœºå°åƒã€æ€€çŸ³æ–™ç†ã€æŠ¹èŒ¶ç”œå“ã€‚", "turn": 8, "days_ago": 8},

        # ç¬¬ 9-12 è½®ï¼ˆ7-4 å¤©å‰ï¼‰- æ›´å¤šç»†èŠ‚
        {"role": "user", "content": "äº¤é€šæ–¹é¢éœ€è¦ä¹° JR Pass å—ï¼Ÿ", "turn": 9, "days_ago": 7},
        {"role": "assistant", "content": "ä¸œäº¬-äº¬éƒ½å¾€è¿”åŠ ä¸Šå¸‚å†…äº¤é€šï¼Œ7 æ—¥ JR Passï¼ˆçº¦ 1800 å…ƒï¼‰æ¯”è¾ƒåˆ’ç®—ã€‚ä¹Ÿå¯ä»¥è€ƒè™‘ ICOCA å¡ç”¨äºå¸‚å†…äº¤é€šã€‚", "turn": 9, "days_ago": 7},
        {"role": "user", "content": "è¯­è¨€ä¸é€šæ€ä¹ˆåŠï¼Ÿ", "turn": 10, "days_ago": 6},
        {"role": "assistant", "content": "æ—¥æœ¬ä¸»è¦æ™¯ç‚¹å’Œè½¦ç«™éƒ½æœ‰ä¸­è‹±æ–‡æ ‡è¯†ã€‚å»ºè®®ä¸‹è½½ Google ç¿»è¯‘å’Œæ¢ä¹˜æ¡ˆå†… Appï¼Œå‡†å¤‡ä¸€äº›å¸¸ç”¨æ—¥è¯­çŸ­å¥ã€‚", "turn": 10, "days_ago": 6},
        {"role": "user", "content": "æœ‰ä»€ä¹ˆéœ€è¦æ³¨æ„çš„ç¤¼ä»ªå—ï¼Ÿ", "turn": 11, "days_ago": 5},
        {"role": "assistant", "content": "ä¸»è¦æ³¨æ„ï¼šç”µè½¦ä¸Šä¸è¦å¤§å£°è¯´è¯ã€åƒåœ¾åˆ†ç±»ã€è¿›å±‹è„±é‹ã€æ³¡æ¸©æ³‰å‰è¦å…ˆæ´—æ¾¡ã€ä¸è¦è¾¹èµ°è¾¹åƒã€‚", "turn": 11, "days_ago": 5},
        {"role": "user", "content": "è´­ç‰©å¯ä»¥é€€ç¨å—ï¼Ÿ", "turn": 12, "days_ago": 4},
        {"role": "assistant", "content": "å¯ä»¥ã€‚å•æ¬¡è´­ä¹°æ»¡ 5000 æ—¥å…ƒï¼ˆçº¦ 250 å…ƒï¼‰å¯ä»¥åŠç†é€€ç¨ï¼Œé€€ç¨ç‡ 8%ã€‚éœ€è¦å¸¦æŠ¤ç…§ï¼Œåœ¨æŒ‡å®šæŸœå°åŠç†ã€‚", "turn": 12, "days_ago": 4},

        # ç¬¬ 13-16 è½®ï¼ˆ3-1 å¤©å‰ï¼‰- è¿›ä¸€æ­¥ç¡®è®¤
        {"role": "user", "content": "å¸¦å¤šå°‘ç°é‡‘åˆé€‚ï¼Ÿ", "turn": 13, "days_ago": 3},
        {"role": "assistant", "content": "å»ºè®®å¸¦ 3-5 ä¸‡æ—¥å…ƒç°é‡‘ï¼ˆçº¦ 1500-2500 å…ƒï¼‰ï¼Œç”¨äºå°åº—æ¶ˆè´¹å’Œäº¤é€šã€‚å¤šæ•°åœ°æ–¹æ”¯æŒä¿¡ç”¨å¡ã€‚", "turn": 13, "days_ago": 3},
        {"role": "user", "content": "éœ€è¦ä¹°ä¿é™©å—ï¼Ÿ", "turn": 14, "days_ago": 2},
        {"role": "assistant", "content": "å»ºè®®è´­ä¹°æ—…æ¸¸æ„å¤–é™©ï¼Œæ¶µç›–åŒ»ç–—ã€è¡Œæä¸¢å¤±ã€èˆªç­å»¶è¯¯ç­‰ã€‚è´¹ç”¨çº¦ 100-200 å…ƒã€‚", "turn": 14, "days_ago": 2},
        {"role": "user", "content": "æ‰‹æœºä¸Šç½‘æ€ä¹ˆè§£å†³ï¼Ÿ", "turn": 15, "days_ago": 1},
        {"role": "assistant", "content": "å¯ä»¥ç§Ÿç§»åŠ¨ WiFiï¼ˆçº¦ 10 å…ƒ/å¤©ï¼‰æˆ–ä¹°ç”µè¯å¡ï¼ˆ7 å¤©çº¦ 80 å…ƒï¼‰ã€‚æœºåœºå’Œä¾¿åˆ©åº—éƒ½æœ‰å”®ã€‚", "turn": 15, "days_ago": 1},
        {"role": "user", "content": "è¿˜æœ‰ä»€ä¹ˆå…¶ä»–å»ºè®®å—ï¼Ÿ", "turn": 16, "days_ago": 1},
        {"role": "assistant", "content": "è®°å¾—ä¸‹è½½å¥½ç¦»çº¿åœ°å›¾ï¼Œæå‰é¢„è®¢çƒ­é—¨é¤å…ï¼Œå‡†å¤‡ä¸€äº› 1 å…ƒç¡¬å¸ç”¨äºæŠ•å¸æœºã€‚ç¥æ—…é€”æ„‰å¿«ï¼", "turn": 16, "days_ago": 1},

        # ç¬¬ 17-18 è½®ï¼ˆä»Šå¤©æ—©ä¸Šï¼‰- æ–°é—®é¢˜
        {"role": "user", "content": "å¯¹äº†ï¼Œå¯Œå£«å±±å€¼å¾—å»å—ï¼Ÿ", "turn": 17, "days_ago": 0},
        {"role": "assistant", "content": "å€¼å¾—ï¼ä»ä¸œäº¬å‡ºå‘ï¼Œå¯ä»¥å»æ²³å£æ¹–çœ‹å¯Œå£«å±±ï¼Œæˆ–è€…åç™»å±±ç”µè½¦åˆ°äº”åˆç›®ã€‚5 æœˆå¤©æ°”å¥½çš„è¯èƒ½çœ‹åˆ°é›ªé¡¶ã€‚", "turn": 17, "days_ago": 0},
    ]

    # æ˜¾ç¤ºå®Œæ•´å†å²
    history_table = create_comparison_table(
        "å®Œæ•´å¯¹è¯å†å²ï¼ˆ20 è½®ï¼Œ38 æ¡æ¶ˆæ¯ï¼‰",
        ["è½®æ¬¡", "è§’è‰²", "å†…å®¹é¢„è§ˆ", "æ—¶é—´", "æ ‡è®°"],
        [
            [
                str(msg.get("turn", "?")),
                msg["role"],
                truncate_text(msg["content"], 45),
                f"{msg.get('days_ago', 0)} å¤©å‰",
                "ğŸ”’ é‡è¦" if msg.get("must_keep") else "",
            ]
            for msg in conversation_history[:10]  # åªæ˜¾ç¤ºå‰ 10 æ¡
        ] + [["...", "...", "è¿˜æœ‰ 28 æ¡æ¶ˆæ¯", "...", "..."]]
    )
    console.print(history_table)

    # è®¡ç®—åŸå§‹ Token æ•°
    raw_tokens = sum(len(msg["content"]) // 4 for msg in conversation_history)
    console.print(f"\nåŸå§‹å†å²æ€» Tokenï¼š[bold]{format_tokens(raw_tokens)}[/bold]\n")

    print_section("æ­¥éª¤ 2ï¼šä½¿ç”¨ Context Forge å‹ç¼©å†å²")

    # åˆ›å»º ContextForge å®ä¾‹ï¼ˆå°çª—å£ï¼Œè§¦å‘å‹ç¼©ï¼‰
    forge = ContextForge(
        model="gpt-4o",
        max_context_tokens=4096,  # å°çª—å£
        output_reserved_tokens=512,
    )

    # å¯ç”¨å‹ç¼©å’Œæ—¶æ•ˆæ€§åŠ æƒ
    from context_forge.config.schema import CompressConfig, RerankConfig

    # [Design Decision] é»˜è®¤ä½¿ç”¨æˆªæ–­å‹ç¼©ï¼ˆé›¶ä¾èµ–ï¼‰ï¼Œç”Ÿäº§ç¯å¢ƒå¯åˆ‡æ¢ä¸º LLM æ‘˜è¦å‹ç¼©ï¼š
    #   default_compressor="summary"  â€” éœ€è¦é…ç½® LLM Providerï¼ˆå¦‚ OPENAI_API_KEYï¼‰
    #   è¯¦è§ configs/conversation_policy.yaml ä¸­çš„å®Œæ•´é…ç½®
    new_compress = CompressConfig(
        enabled=True,
        default_compressor="truncation",  # æˆªæ–­å‹ç¼©ï¼ˆæ— éœ€ LLMï¼‰ï¼›æ”¹ä¸º "summary" å¯ç”¨ Rolling Summary
        saturation_trigger=0.75,
        preserve_must_keep=True,
        min_segment_tokens=50,
    )

    new_rerank = RerankConfig(
        enable_mmr=False,
        enable_temporal_weighting=True,
        temporal_decay_rate=0.2,  # ä¸­ç­‰è¡°å‡
        temporal_min_weight=0.2,
    )

    forge._policy = forge._policy.model_copy(update={
        "compress": new_compress,
        "rerank": new_rerank,
    })

    # é‡æ–°åˆ›å»º pipeline
    from context_forge.pipeline.base import create_default_pipeline
    forge._pipeline = create_default_pipeline(policy=forge._policy)

    # å‡†å¤‡ Segment åˆ—è¡¨ï¼ˆæ‰‹åŠ¨åˆ›å»ºä»¥ä¾¿æ·»åŠ  must_keep æ ‡è®°ï¼‰
    segments = []
    for msg in conversation_history:
        days_ago = msg.get("days_ago", 0)
        timestamp = now - timedelta(days=days_ago)

        seg_type = SegmentType.USER if msg["role"] == "user" else SegmentType.ASSISTANT
        priority = Priority.HIGH if msg.get("must_keep") else Priority.MEDIUM

        segments.append(Segment(
            type=seg_type,
            content=msg["content"],
            role=msg["role"],
            priority=priority,
            control=ControlFlags(
                must_keep=msg.get("must_keep", False),
                compressible=not msg.get("must_keep", False),
            ),
            provenance=Provenance(
                source_id=f"turn_{msg.get('turn', 0)}",
                source_type=SourceType.USER_INPUT if msg["role"] == "user" else SourceType.SYSTEM_CONFIG,
                retrieved_at=timestamp,
            ),
            metadata=SegmentMetadata(
                turn_number=msg.get("turn", 0),
                custom={"conversation_timestamp": timestamp.isoformat()},
            ),
        ))

    # ç»„è£…ä¸Šä¸‹æ–‡ï¼ˆå½“å‰é—®é¢˜ï¼šè®¡åˆ’å¯Œå£«å±±è¡Œç¨‹ï¼‰
    context = await forge.build(
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡Œè§„åˆ’åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·åˆ¶å®šæ—¥æœ¬æ—…è¡Œè®¡åˆ’ã€‚",
        messages=[{"role": "user", "content": "æ ¹æ®æˆ‘ä¹‹å‰çš„éœ€æ±‚ï¼Œå¸®æˆ‘è§„åˆ’ä¸€ä¸‹å¯Œå£«å±±ä¸€æ—¥æ¸¸çš„è¡Œç¨‹ã€‚"}],
        extra_segments=segments,
        current_turn=19,
    )

    print_success(f"ä¸Šä¸‹æ–‡ç»„è£…å®Œæˆï¼Œè€—æ—¶ {context.assembly_duration_ms:.1f}ms\n")

    print_section("æ­¥éª¤ 3ï¼šå‹ç¼©ä¸è¿‡æ»¤ç»“æœ")

    # ç»Ÿè®¡ä¿ç•™çš„å¯¹è¯
    from context_forge.models.segment import SegmentType
    conversation_segments = [s for s in context.segments if s.type in (SegmentType.USER, SegmentType.ASSISTANT)]
    kept_count = len(conversation_segments)
    dropped_count = len(conversation_history) - kept_count
    must_keep_count = len([s for s in conversation_segments if s.control.must_keep])

    console.print(f"åŸå§‹æ¶ˆæ¯ï¼š[bold]{len(conversation_history)}[/bold] æ¡")
    console.print(f"ä¿ç•™æ¶ˆæ¯ï¼š[bold green]{kept_count}[/bold green] æ¡ï¼ˆå« {must_keep_count} æ¡å…³é”®ä¿¡æ¯ï¼‰")
    console.print(f"ä¸¢å¼ƒæ¶ˆæ¯ï¼š[bold red]{dropped_count}[/bold red] æ¡ï¼ˆä½æ—¶æ•ˆæ€§ + ä½ä¼˜å…ˆçº§ï¼‰\n")

    # æ˜¾ç¤ºä¿ç•™çš„æ¶ˆæ¯
    kept_table = create_comparison_table(
        "ä¿ç•™çš„å¯¹è¯ç‰‡æ®µï¼ˆæŒ‰æ—¶æ•ˆæ€§åŠ æƒæ’åºï¼‰",
        ["è½®æ¬¡", "è§’è‰²", "å†…å®¹é¢„è§ˆ", "ä¼˜å…ˆçº§", "Token"],
        [
            [
                seg.metadata.turn_number if seg.metadata else "?",
                seg.role,
                truncate_text(seg.content, 45),
                seg.priority.value + (" ğŸ”’" if seg.control.must_keep else ""),
                str(seg.token_count or 0),
            ]
            for seg in conversation_segments[:8]  # åªæ˜¾ç¤ºå‰ 8 æ¡
        ]
    )
    console.print(kept_table)

    if kept_count > 8:
        console.print(f"\n   ... è¿˜æœ‰ {kept_count - 8} æ¡æ¶ˆæ¯\n")

    print_section("æ­¥éª¤ 4ï¼šå®¡è®¡æ—¥å¿— â€” å‹ç¼©å†³ç­–")

    # æ˜¾ç¤ºå‹ç¼©å’Œé‡æ’å†³ç­–
    compress_entries = [e for e in context.audit_log if "å‹ç¼©" in e.reason_detail or "ä¸¢å¼ƒ" in e.reason_detail or "æ—¶æ•ˆ" in e.reason_detail]

    console.print("[bold]å‹ç¼©é˜¶æ®µå†³ç­–ï¼š[/bold]\n")
    for entry in compress_entries[:6]:
        console.print(f"  - [dim]{entry.segment_id}[/dim]: {entry.reason_detail}")

    # æ˜¾ç¤º must_keep ä¿æŠ¤
    must_keep_entries = [e for e in context.audit_log if "must_keep" in e.reason_detail.lower() or "ä¿æŠ¤" in e.reason_detail]
    if must_keep_entries:
        console.print("\n[bold green]å…³é”®ä¿¡æ¯ä¿æŠ¤ï¼š[/bold green]\n")
        for entry in must_keep_entries[:3]:
            console.print(f"  - [dim]{entry.segment_id}[/dim]: {entry.reason_detail}")

    print_section("æ­¥éª¤ 5ï¼šToken ä¼˜åŒ–æ•ˆæœ")

    # è®¡ç®— Token èŠ‚çœ
    compressed_tokens = sum(s.token_count or 0 for s in conversation_segments)
    saved_tokens = raw_tokens - compressed_tokens
    saved_ratio = saved_tokens / raw_tokens if raw_tokens > 0 else 0

    # è·å–é¢„ç®—ä½¿ç”¨æƒ…å†µ
    budget_allocation = context.budget_allocation

    metrics_table = create_comparison_table(
        "ä¼˜åŒ–æŒ‡æ ‡",
        ["æŒ‡æ ‡", "å‹ç¼©å‰", "å‹ç¼©å", "ä¼˜åŒ–"],
        [
            [
                "å¯¹è¯æ¶ˆæ¯æ•°",
                str(len(conversation_history)),
                str(kept_count),
                f"-{dropped_count} ({format_percentage(dropped_count / len(conversation_history))})",
            ],
            [
                "Token æ€»æ•°",
                format_tokens(raw_tokens),
                format_tokens(compressed_tokens),
                f"-{format_tokens(saved_tokens)} ({format_percentage(saved_ratio)})",
            ],
            [
                "é¢„ç®—é¥±å’Œåº¦",
                "N/A",
                format_percentage(budget_allocation.saturation_rate),
                "OK å¥åº·" if budget_allocation.saturation_rate < 0.85 else "! æ¥è¿‘ä¸Šé™",
            ],
            [
                "å…³é”®ä¿¡æ¯ä¿ç•™",
                f"{must_keep_count} æ¡",
                f"{must_keep_count} æ¡",
                "OK 100% ä¿æŠ¤",
            ],
        ]
    )
    console.print(metrics_table)

    print_section("æ­¥éª¤ 6ï¼šæ»‘åŠ¨çª—å£ç­–ç•¥")

    # åˆ†ææ—¶é—´åˆ†å¸ƒ
    console.print("[bold]å¯¹è¯æ—¶é—´åˆ†å¸ƒåˆ†æï¼š[/bold]\n")

    # æŒ‰æ—¶é—´æ®µç»Ÿè®¡ï¼ˆä½¿ç”¨ provenance.retrieved_at è®°å½•çš„å¯¹è¯æ—¶é—´ï¼‰
    recent_7d = [s for s in conversation_segments if s.provenance and s.provenance.retrieved_at and (now - s.provenance.retrieved_at).days < 7]
    recent_14d = [s for s in conversation_segments if s.provenance and s.provenance.retrieved_at and 7 <= (now - s.provenance.retrieved_at).days < 14]
    older = [s for s in conversation_segments if s.provenance and s.provenance.retrieved_at and (now - s.provenance.retrieved_at).days >= 14]

    distribution_table = create_comparison_table(
        "æ—¶é—´åˆ†å¸ƒ",
        ["æ—¶é—´æ®µ", "ä¿ç•™æ¶ˆæ¯æ•°", "å æ¯”", "è¯´æ˜"],
        [
            ["æœ€è¿‘ 7 å¤©", str(len(recent_7d)), format_percentage(len(recent_7d) / kept_count), "é«˜æ—¶æ•ˆæ€§"],
            ["7-14 å¤©", str(len(recent_14d)), format_percentage(len(recent_14d) / kept_count), "ä¸­æ—¶æ•ˆæ€§"],
            ["14 å¤©ä»¥ä¸Š", str(len(older)), format_percentage(len(older) / kept_count), "ä½æ—¶æ•ˆæ€§ï¼ˆmust_keep é™¤å¤–ï¼‰"],
        ]
    )
    console.print(distribution_table)

    print_section("æ€»ç»“")

    print_success(f"å¯¹è¯è®°å¿†ç®¡ç†å®Œæˆï¼")
    print_success(f"- ä» {len(conversation_history)} æ¡æ¶ˆæ¯å‹ç¼©åˆ° {kept_count} æ¡ï¼Œä¿ç•™ç‡ {format_percentage(kept_count / len(conversation_history))}")
    print_success(f"- èŠ‚çœäº† {format_percentage(saved_ratio)} çš„ Token å¼€é”€")
    print_success(f"- {must_keep_count} æ¡å…³é”®ä¿¡æ¯ï¼ˆé¢„ç®—ã€æ—¶é—´ã€å…´è¶£ï¼‰100% ä¿æŠ¤")
    print_success(f"- æ—¶æ•ˆæ€§åŠ æƒç¡®ä¿æœ€è¿‘å¯¹è¯ä¼˜å…ˆä¿ç•™")
    print_success(f"- é¢„ç®—é¥±å’Œåº¦ï¼š{format_percentage(budget_allocation.saturation_rate)} (å¥åº·)")

    console.print(f"\n[dim]æç¤ºï¼šç”Ÿäº§ç¯å¢ƒå»ºè®®å¯ç”¨ RollingSummaryCompressor è¿›è¡Œå¢é‡æ‘˜è¦å‹ç¼©[/dim]")

    # ==========================================
    # é™„åŠ æ¼”ç¤ºï¼šRollingSummaryCompressor æ»šåŠ¨æ‘˜è¦
    # ==========================================
    print_section("é™„åŠ æ¼”ç¤ºï¼šRollingSummaryCompressor æ»šåŠ¨æ‘˜è¦")

    console.print(
        "[bold]RollingSummaryCompressor ä¸ LLMSummaryCompressor çš„åŒºåˆ«ï¼š[/bold]\n"
        "  - æœ‰çŠ¶æ€ï¼šè·¨ build() è°ƒç”¨ä¿ç•™å†å²æ‘˜è¦\n"
        "  - å¢é‡æ›´æ–°ï¼š\"ä¸Šè½®æ‘˜è¦ + æ–°æ¶ˆæ¯ â†’ æ›´æ–°æ‘˜è¦\"\n"
        "  - è½®æ¬¡æ„ŸçŸ¥ï¼šæœ€è¿‘ N è½®ä¿æŒåŸæ–‡ï¼Œä»…æ‘˜è¦æ›´æ—©çš„è½®æ¬¡\n"
    )

    from context_forge.compress.summary import RollingSummaryCompressor
    from context_forge.compress.base import CompressContext

    # åˆ›å»º Mock LLM Providerï¼ˆå®ç° LLMProvider Protocolï¼‰
    class DemoLLMProvider:
        """æ¼”ç¤ºç”¨ Mock LLM Providerã€‚"""
        def __init__(self):
            self.call_count = 0

        async def generate(self, prompt: str, max_tokens: int = 500) -> str:
            self.call_count += 1
            if "ä¸Šä¸€è½®æ‘˜è¦" in prompt:
                return (
                    "æ›´æ–°æ‘˜è¦ï¼š\n"
                    "1. ç”¨æˆ·è®¡åˆ’ 5 æœˆå»æ—¥æœ¬æ—…è¡Œï¼Œé¢„ç®— 2 ä¸‡å…ƒï¼Œ7-10 å¤©\n"
                    "2. ç›®çš„åœ°ï¼šä¸œäº¬ï¼ˆ4 å¤©ï¼‰+ äº¬éƒ½ï¼ˆ3 å¤©ï¼‰ï¼Œå¯åŠ å¤§é˜ª\n"
                    "3. å…´è¶£ï¼šåŠ¨æ¼«ã€ç”µå­äº§å“ï¼ˆç§‹å¶åŸã€ä¸­é‡ç™¾è€æ±‡ï¼‰\n"
                    "4. ä½å®¿/äº¤é€š/ç¾é£Ÿç­‰å·²å’¨è¯¢å®Œæ¯•\n"
                    "5. æ–°å¢ï¼šç”¨æˆ·è¯¢é—®å¯Œå£«å±±ä¸€æ—¥æ¸¸"
                )
            return (
                "åˆå§‹æ‘˜è¦ï¼š\n"
                "1. ç”¨æˆ·è®¡åˆ’ 5 æœˆå»æ—¥æœ¬æ—…è¡Œï¼Œé¢„ç®— 2 ä¸‡å…ƒï¼Œ7-10 å¤©\n"
                "2. ç›®çš„åœ°ï¼šä¸œäº¬ + äº¬éƒ½\n"
                "3. å…´è¶£ï¼šåŠ¨æ¼«ã€ç”µå­äº§å“"
            )

    demo_provider = DemoLLMProvider()
    rolling_compressor = RollingSummaryCompressor(
        provider=demo_provider,
        keep_recent_turns=2,  # ä¿ç•™æœ€è¿‘ 2 è½®åŸæ–‡
        max_summary_tokens=300,
    )

    compress_ctx = CompressContext(
        available_tokens=4096,
        target_token_count=2048,
        saturation=0.9,
    )

    # ç¬¬ä¸€è½®ï¼šåˆå§‹æ‘˜è¦ï¼ˆæ¨¡æ‹Ÿå‰ 15 è½®å¯¹è¯ï¼‰
    print_section("Rolling ç¬¬ 1 è½®ï¼šåˆå§‹æ‘˜è¦")
    console.print("[dim]è¾“å…¥ 15 è½®å¯¹è¯å†å²ï¼Œkeep_recent_turns=2[/dim]\n")

    round1_segments = [
        Segment(
            type=SegmentType.USER if msg["role"] == "user" else SegmentType.ASSISTANT,
            content=msg["content"],
            role=msg["role"],
            token_count=len(msg["content"]) // 2,
            metadata=SegmentMetadata(turn_number=msg.get("turn", 0)),
        )
        for msg in conversation_history[:30]  # å‰ 15 è½®ï¼ˆ30 æ¡æ¶ˆæ¯ï¼‰
    ]

    result1 = await rolling_compressor.compress(round1_segments, compress_ctx)

    console.print(f"  è¾“å…¥ Segment æ•°ï¼š{len(round1_segments)}")
    console.print(f"  è¾“å‡º Segment æ•°ï¼š{len(result1.compressed_segments)}")
    console.print(f"  LLM è°ƒç”¨æ¬¡æ•°ï¼š{demo_provider.call_count}")
    console.print(f"  æ»šåŠ¨çŠ¶æ€ï¼š{result1.metadata['rolling_state']}")
    console.print(f"  æ‘˜è¦å†…å®¹ä¿ç•™çš„æœ€è¿‘è½®æ¬¡æ•°ï¼š{result1.metadata['recent_count']}")
    console.print(f"  has_state = {rolling_compressor.has_state}")

    # æ˜¾ç¤ºæ‘˜è¦å†…å®¹
    summary_seg = result1.compressed_segments[0]
    console.print(f"\n[bold green]ç”Ÿæˆçš„æ‘˜è¦ï¼š[/bold green]")
    console.print(f"  {summary_seg.content}")

    # æ˜¾ç¤ºä¿ç•™çš„æœ€è¿‘è½®æ¬¡
    recent_segs = result1.compressed_segments[1:]
    console.print(f"\n[bold cyan]ä¿ç•™çš„æœ€è¿‘è½®æ¬¡åŸæ–‡ï¼ˆ{len(recent_segs)} æ¡ï¼‰ï¼š[/bold cyan]")
    for seg in recent_segs[:4]:
        console.print(f"  [{seg.role}] {truncate_text(seg.content, 50)}")

    # ç¬¬äºŒè½®ï¼šå¢é‡æ›´æ–°ï¼ˆæ–°å¢ 2 è½®å¯¹è¯ï¼‰
    print_section("Rolling ç¬¬ 2 è½®ï¼šå¢é‡æ›´æ–°")
    console.print("[dim]æ–°å¢ 2 è½®å¯¹è¯ï¼Œæ»šåŠ¨æ‘˜è¦å¢é‡æ›´æ–°[/dim]\n")

    round2_segments = round1_segments + [
        Segment(
            type=SegmentType.USER, content="å¯¹äº†ï¼Œå¯Œå£«å±±å€¼å¾—å»å—ï¼Ÿ",
            role="user", token_count=10,
            metadata=SegmentMetadata(turn_number=17),
        ),
        Segment(
            type=SegmentType.ASSISTANT,
            content="å€¼å¾—ï¼ä»ä¸œäº¬å‡ºå‘ï¼Œå¯ä»¥å»æ²³å£æ¹–çœ‹å¯Œå£«å±±ã€‚",
            role="assistant", token_count=15,
            metadata=SegmentMetadata(turn_number=17),
        ),
        Segment(
            type=SegmentType.USER, content="æ€ä¹ˆå®‰æ’å¯Œå£«å±±ä¸€æ—¥æ¸¸ï¼Ÿ",
            role="user", token_count=10,
            metadata=SegmentMetadata(turn_number=18),
        ),
        Segment(
            type=SegmentType.ASSISTANT,
            content="å»ºè®®æ—©ä¸Šä»æ–°å®¿å‡ºå‘ï¼Œåå¤§å·´åˆ°æ²³å£æ¹–ï¼Œæ¸¸ç©åå‚æ™šè¿”å›ã€‚",
            role="assistant", token_count=20,
            metadata=SegmentMetadata(turn_number=18),
        ),
    ]

    result2 = await rolling_compressor.compress(round2_segments, compress_ctx)

    console.print(f"  è¾“å…¥ Segment æ•°ï¼š{len(round2_segments)}")
    console.print(f"  è¾“å‡º Segment æ•°ï¼š{len(result2.compressed_segments)}")
    console.print(f"  LLM è°ƒç”¨æ¬¡æ•°ï¼ˆç´¯è®¡ï¼‰ï¼š{demo_provider.call_count}")
    console.print(f"  æ»šåŠ¨çŠ¶æ€ï¼š{result2.metadata['rolling_state']}")

    summary_seg2 = result2.compressed_segments[0]
    console.print(f"\n[bold green]æ›´æ–°åçš„æ‘˜è¦ï¼š[/bold green]")
    console.print(f"  {summary_seg2.content}")

    recent_segs2 = result2.compressed_segments[1:]
    console.print(f"\n[bold cyan]ä¿ç•™çš„æœ€è¿‘è½®æ¬¡åŸæ–‡ï¼ˆ{len(recent_segs2)} æ¡ï¼‰ï¼š[/bold cyan]")
    for seg in recent_segs2:
        console.print(f"  [{seg.role}] {truncate_text(seg.content, 50)}")

    # é‡ç½®æ¼”ç¤º
    print_section("Rolling çŠ¶æ€ç®¡ç†")
    console.print(f"  reset å‰ has_state = {rolling_compressor.has_state}")
    rolling_compressor.reset()
    console.print(f"  reset å has_state = {rolling_compressor.has_state}")

    # Token èŠ‚çœç»Ÿè®¡
    original_total = sum(seg.token_count or 0 for seg in round2_segments)
    compressed_total = result2.compressed_token_count
    saved = original_total - compressed_total
    saved_pct = saved / original_total if original_total > 0 else 0

    console.print(f"\n[bold]æ»šåŠ¨æ‘˜è¦ Token ä¼˜åŒ–ï¼š[/bold]")
    console.print(f"  åŸå§‹ Tokenï¼š{format_tokens(original_total)}")
    console.print(f"  å‹ç¼©å Tokenï¼š{format_tokens(compressed_total)}")
    console.print(f"  èŠ‚çœï¼š{format_tokens(saved)} ({format_percentage(saved_pct)})")

    print_success("æ»šåŠ¨æ‘˜è¦æ¼”ç¤ºå®Œæˆï¼")
    print_success("- æ”¯æŒè·¨è°ƒç”¨å¢é‡æ›´æ–°ï¼Œæ— éœ€æ¯æ¬¡é‡æ–°æ‘˜è¦å…¨éƒ¨å†å²")
    print_success("- æœ€è¿‘è½®æ¬¡ä¿æŒåŸæ–‡ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡è¿è´¯æ€§")
    print_success("- reset() å¯éšæ—¶æ¸…é™¤çŠ¶æ€ï¼Œé‡æ–°å¼€å§‹")


if __name__ == "__main__":
    args = parse_args("åœºæ™¯ 2ï¼šå¤šè½®å¯¹è¯è®°å¿†ç®¡ç†")
    asyncio.run(main(mock=args.mock))
