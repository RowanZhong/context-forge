"""
ç¼“å­˜æ¨¡å—æ¼”ç¤º â€” æå‡ä¸Šä¸‹æ–‡ç»„è£…æ€§èƒ½ã€‚

â†’ 6.2.3 ç¼“å­˜æ¶æ„ä¸å¤ç”¨ä¼˜åŒ–

æ¼”ç¤ºåœºæ™¯ï¼š
1. Segment çº§ç¼“å­˜ï¼šå¤ç”¨æ¸…æ´—åçš„å†…å®¹
2. Prefix çº§ç¼“å­˜ï¼šå…±äº«é™æ€ System Segment
3. Context çº§ç¼“å­˜ï¼šè·³è¿‡æ•´ä¸ª Pipeline
4. L1 + L2 åˆ†å±‚ç¼“å­˜ï¼šè·¨è¿›ç¨‹å¤ç”¨

è¿è¡Œæ–¹å¼ï¼š
    python examples/cache_demo.py
"""

import asyncio
import json
import sys
from datetime import datetime

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç 
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8")

from context_forge.cache import (
    CacheEntry,
    CacheManager,
    MemoryCache,
    compute_context_key,
    compute_prefix_key,
    compute_segment_key,
    create_default_cache,
)


def print_section(title: str) -> None:
    """æ‰“å°ç« èŠ‚æ ‡é¢˜ã€‚"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}\n")


async def demo_segment_cache():
    """æ¼”ç¤º Segment çº§ç¼“å­˜ã€‚"""
    print_section("åœºæ™¯ 1ï¼šSegment çº§ç¼“å­˜ â€” å¤ç”¨æ¸…æ´—ç»“æœ")

    cache = create_default_cache(l1_max_size=1000, default_ttl=3600)

    # æ¨¡æ‹Ÿæ¸…æ´—è¿‡çš„ Segment
    original_content = "Python's GIL has been removed in 3.13"
    cleaned_content = "Python's GIL has been removed in 3.13"  # å‡è®¾æ¸…æ´—åæ— å˜åŒ–
    model = "gpt-4o"

    # è®¡ç®—ç¼“å­˜é”®
    cache_key = compute_segment_key(original_content, model)
    print(f"ç¼“å­˜é”®: {cache_key}")

    # é¦–æ¬¡è®¿é—®ï¼šæœªå‘½ä¸­ï¼Œéœ€è¦æ¸…æ´—
    cached = await cache.get(cache_key)
    if cached is None:
        print("âŒ ç¼“å­˜æœªå‘½ä¸­ â€” æ‰§è¡Œæ¸…æ´—æ“ä½œ...")
        # æ¨¡æ‹Ÿæ¸…æ´—è¿‡ç¨‹ï¼ˆå®é™…ä¼šè°ƒç”¨ Sanitizerï¼‰
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæ¸…æ´—å»¶è¿Ÿ
        cleaned_data = {
            "content": cleaned_content,
            "sanitized": True,
            "timestamp": datetime.utcnow().isoformat(),
        }
        entry = CacheEntry(value=json.dumps(cleaned_data))
        await cache.set(cache_key, entry, ttl=3600)
        print(f"âœ“ æ¸…æ´—å®Œæˆå¹¶ç¼“å­˜: {cleaned_data}")
    else:
        print(f"âœ“ ç¼“å­˜å‘½ä¸­: {cached.value}")

    # å†æ¬¡è®¿é—®ï¼šå‘½ä¸­ç¼“å­˜
    print("\nç¬¬äºŒæ¬¡è®¿é—®ç›¸åŒå†…å®¹...")
    cached = await cache.get(cache_key)
    if cached is not None:
        data = json.loads(cached.value)
        print(f"âœ“ ç¼“å­˜å‘½ä¸­ï¼ˆå‘½ä¸­æ¬¡æ•°: {cached.hit_count}ï¼‰")
        print(f"  å†…å®¹: {data['content']}")
        print(f"  ç¼“å­˜æ—¶é—´: {data['timestamp']}")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = await cache.stats()
    print(f"\nç¼“å­˜ç»Ÿè®¡: å‘½ä¸­ç‡={stats['l1'].hit_rate:.1%}, "
          f"å‘½ä¸­={stats['l1'].hits}, æœªå‘½ä¸­={stats['l1'].misses}")


async def demo_prefix_cache():
    """æ¼”ç¤º Prefix çº§ç¼“å­˜ã€‚"""
    print_section("åœºæ™¯ 2ï¼šPrefix çº§ç¼“å­˜ â€” å…±äº«é™æ€ System Segment")

    cache = create_default_cache()

    # é™æ€ System Segment åºåˆ—ï¼ˆå¤šè½®å¯¹è¯ä¸­ä¸å˜ï¼‰
    system_segments = [
        "You are a helpful AI assistant.",
        "Always respond in Chinese.",
        "Be concise and accurate.",
    ]
    model = "claude-sonnet-4-5-20250514"
    policy_version = "v1.0"

    # è®¡ç®— Prefix ç¼“å­˜é”®
    prefix_key = compute_prefix_key(system_segments, model, policy_version)
    print(f"Prefix ç¼“å­˜é”®: {prefix_key}")

    # æ¨¡æ‹Ÿ KV Cache å‰ç¼€
    cached = await cache.get(prefix_key)
    if cached is None:
        print("âŒ Prefix ç¼“å­˜æœªå‘½ä¸­ â€” ç”Ÿæˆ KV Cache...")
        await asyncio.sleep(0.2)  # æ¨¡æ‹Ÿ LLM é¢„å¡«å……å»¶è¿Ÿ
        kv_cache_data = {
            "model": model,
            "segments": system_segments,
            "kv_cache_size_mb": 1.2,
            "created_at": datetime.utcnow().isoformat(),
        }
        entry = CacheEntry(value=json.dumps(kv_cache_data))
        await cache.set(prefix_key, entry, ttl=7200)  # System Segment è¾ƒé•¿ TTL
        print(f"âœ“ KV Cache å·²ç”Ÿæˆå¹¶ç¼“å­˜: {kv_cache_data['kv_cache_size_mb']} MB")
    else:
        data = json.loads(cached.value)
        print(f"âœ“ Prefix ç¼“å­˜å‘½ä¸­ â€” å¤ç”¨ KV Cache")
        print(f"  System Segments: {data['segments']}")
        print(f"  KV Cache å¤§å°: {data['kv_cache_size_mb']} MB")
        print(f"  åˆ›å»ºæ—¶é—´: {data['created_at']}")

    print("\nğŸ’¡ æç¤º: åœ¨å¤šè½®å¯¹è¯ä¸­ï¼Œé™æ€ System Segment çš„ Prefix ç¼“å­˜")
    print("   å¯ä»¥æ˜¾è‘—å‡å°‘ LLM é¢„å¡«å……æ—¶é—´ï¼ˆRadixAttention ä¼˜åŒ–ï¼‰ã€‚")


async def demo_context_cache():
    """æ¼”ç¤º Context çº§ç¼“å­˜ã€‚"""
    print_section("åœºæ™¯ 3ï¼šContext çº§ç¼“å­˜ â€” è·³è¿‡æ•´ä¸ª Pipeline")

    cache = create_default_cache()

    # å®Œæ•´ä¸Šä¸‹æ–‡ç»“æ„ï¼ˆåºåˆ—åŒ–ä¸º dictï¼‰
    context_segments = [
        {"type": "system", "content": "You are a math tutor.", "priority": "critical"},
        {"type": "few_shot", "content": "Q: 1+1? A: 2", "priority": "high"},
        {"type": "user", "content": "What is 2+2?", "priority": "high"},
    ]
    model = "gpt-4o-mini"
    policy_version = "v2.0"

    # è®¡ç®— Context ç¼“å­˜é”®
    context_key = compute_context_key(context_segments, model, policy_version)
    print(f"Context ç¼“å­˜é”®: {context_key}")

    # é¦–æ¬¡ç»„è£…
    cached = await cache.get(context_key)
    if cached is None:
        print("âŒ Context ç¼“å­˜æœªå‘½ä¸­ â€” æ‰§è¡Œå®Œæ•´ Pipeline...")
        print("   [Normalize â†’ Sanitize â†’ Rerank â†’ Allocate â†’ Assemble]")
        await asyncio.sleep(0.3)  # æ¨¡æ‹Ÿ Pipeline å»¶è¿Ÿ
        assembled_context = {
            "segments": context_segments,
            "total_tokens": 120,
            "model": model,
            "assembled_at": datetime.utcnow().isoformat(),
        }
        entry = CacheEntry(value=json.dumps(assembled_context))
        await cache.set(context_key, entry, ttl=1800)
        print(f"âœ“ Pipeline å®Œæˆ: {assembled_context['total_tokens']} tokens")
    else:
        data = json.loads(cached.value)
        print(f"âœ“ Context ç¼“å­˜å‘½ä¸­ â€” è·³è¿‡æ‰€æœ‰ Pipeline é˜¶æ®µ")
        print(f"  æ€» Token: {data['total_tokens']}")
        print(f"  ç»„è£…æ—¶é—´: {data['assembled_at']}")
        print(f"  å‘½ä¸­æ¬¡æ•°: {cached.hit_count}")

    print("\nğŸ’¡ é€‚ç”¨åœºæ™¯: å›ºå®šçš„ Few-Shot ç¤ºä¾‹ + System Promptï¼ˆå¦‚ JSON è¾“å‡ºæ ¼å¼ï¼‰")
    print("   ä¸é€‚ç”¨åœºæ™¯: åŠ¨æ€ RAG + å¤šè½®å¯¹è¯ï¼ˆæ¯æ¬¡å†…å®¹éƒ½ä¸åŒï¼‰")


async def demo_l1_l2_cache():
    """æ¼”ç¤º L1 + L2 åˆ†å±‚ç¼“å­˜ã€‚"""
    print_section("åœºæ™¯ 4ï¼šL1 + L2 åˆ†å±‚ç¼“å­˜ â€” è·¨è¿›ç¨‹å¤ç”¨")

    # æ³¨æ„ï¼šéœ€è¦ Redis è¿è¡Œåœ¨ localhost:6379
    # å¦‚æœ Redis ä¸å¯ç”¨ï¼Œä¼šè‡ªåŠ¨é™çº§ä¸ºä»… L1
    cache = create_default_cache(
        redis_url="redis://localhost:6379/0",
        l1_max_size=100,
        default_ttl=3600,
    )

    if cache.has_l2:
        print("âœ“ L2 ç¼“å­˜ï¼ˆRedisï¼‰å·²å¯ç”¨")
    else:
        print("âš ï¸  L2 ç¼“å­˜ï¼ˆRedisï¼‰ä¸å¯ç”¨ï¼Œä»…ä½¿ç”¨ L1ï¼ˆå†…å­˜ï¼‰")
        print("   æç¤ºï¼šå®‰è£… Redis å¹¶è¿è¡Œ `redis-server` ä»¥å¯ç”¨ L2 ç¼“å­˜ã€‚")

    # æ¨¡æ‹Ÿè·¨è¿›ç¨‹åœºæ™¯
    key = "shared_segment"
    content = "This segment is shared across workers."

    # Worker 1 å†™å…¥
    print(f"\n[Worker 1] å†™å…¥ç¼“å­˜: {key}")
    entry = CacheEntry(value=json.dumps({"content": content, "worker": 1}))
    await cache.set(key, entry)
    print("âœ“ å·²å†™å…¥ L1 å’Œ L2")

    # æ¨¡æ‹Ÿ Worker 2 è¯»å–ï¼ˆæ¸…ç©º L1ï¼Œå¼ºåˆ¶ä» L2 è¯»å–ï¼‰
    print(f"\n[Worker 2] è¯»å–ç¼“å­˜ï¼ˆL1 å·²æ¸…ç©ºï¼Œæ¨¡æ‹Ÿæ–°è¿›ç¨‹ï¼‰")
    # æ¸…ç©º L1ï¼ˆæ¨¡æ‹Ÿæ–° worker çš„ç©ºç¼“å­˜ï¼‰
    await cache._l1.clear()

    result = await cache.get(key)
    if result is not None:
        data = json.loads(result.value)
        print(f"âœ“ ä» L2 è¯»å–æˆåŠŸ: {data}")
        print(f"  æ¥æº: Worker {data['worker']}")
        print("  â†’ L2 å‘½ä¸­åè‡ªåŠ¨å›å¡« L1ï¼Œåç»­è®¿é—®æ›´å¿«")
    else:
        print("âŒ ç¼“å­˜æœªå‘½ä¸­ï¼ˆå¯èƒ½ Redis æœªè¿è¡Œï¼‰")

    # ç»Ÿè®¡
    stats = await cache.stats()
    print(f"\nç¼“å­˜ç»Ÿè®¡:")
    print(f"  L1: å‘½ä¸­ç‡={stats['l1'].hit_rate:.1%}, å¤§å°={stats['l1'].current_size}")
    if "l2" in stats:
        print(f"  L2: å‘½ä¸­ç‡={stats['l2'].hit_rate:.1%}, å¤§å°={stats['l2'].current_size}")

    await cache.close()


async def demo_performance_comparison():
    """æ€§èƒ½å¯¹æ¯”ï¼šæœ‰ç¼“å­˜ vs æ— ç¼“å­˜ã€‚"""
    print_section("åœºæ™¯ 5ï¼šæ€§èƒ½å¯¹æ¯” â€” ç¼“å­˜åŠ é€Ÿæ•ˆæœ")

    cache = create_default_cache()
    content = "A" * 10000  # 10KB å†…å®¹
    model = "gpt-4o"

    # æ— ç¼“å­˜ï¼šæ¯æ¬¡éƒ½æ‰§è¡Œæ¸…æ´—
    print("æ— ç¼“å­˜åœºæ™¯ï¼ˆ10 æ¬¡æ¸…æ´—ï¼‰...")
    start = asyncio.get_event_loop().time()
    for i in range(10):
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæ¸…æ´—å»¶è¿Ÿ
    no_cache_time = asyncio.get_event_loop().time() - start
    print(f"  æ€»è€—æ—¶: {no_cache_time*1000:.1f} ms")

    # æœ‰ç¼“å­˜ï¼šé¦–æ¬¡æ¸…æ´—ï¼Œåç»­å‘½ä¸­
    print("\næœ‰ç¼“å­˜åœºæ™¯ï¼ˆ1 æ¬¡æ¸…æ´— + 9 æ¬¡å‘½ä¸­ï¼‰...")
    key = compute_segment_key(content, model)
    start = asyncio.get_event_loop().time()

    # é¦–æ¬¡ï¼šæœªå‘½ä¸­
    cached = await cache.get(key)
    if cached is None:
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæ¸…æ´—
        entry = CacheEntry(value=json.dumps({"content": content}))
        await cache.set(key, entry)

    # åç»­ï¼šå‘½ä¸­
    for i in range(9):
        await cache.get(key)

    cache_time = asyncio.get_event_loop().time() - start
    print(f"  æ€»è€—æ—¶: {cache_time*1000:.1f} ms")

    # åŠ é€Ÿæ¯”
    speedup = no_cache_time / cache_time if cache_time > 0 else 0
    print(f"\nğŸ“Š åŠ é€Ÿæ¯”: {speedup:.1f}x")
    print(f"   èŠ‚çœæ—¶é—´: {(no_cache_time - cache_time)*1000:.1f} ms")


async def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤ºã€‚"""
    print("\n" + "="*60)
    print("  Context Forge â€” ç¼“å­˜æ¨¡å—æ¼”ç¤º")
    print("  â†’ 6.2.3 ç¼“å­˜æ¶æ„ä¸å¤ç”¨ä¼˜åŒ–")
    print("="*60)

    await demo_segment_cache()
    await demo_prefix_cache()
    await demo_context_cache()
    await demo_l1_l2_cache()
    await demo_performance_comparison()

    print("\n" + "="*60)
    print("  æ¼”ç¤ºå®Œæˆ âœ“")
    print("="*60 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
