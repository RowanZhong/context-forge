"""
Tokenizer æ³¨å†Œè¡¨ â€” æ ¹æ®æ¨¡åž‹åè‡ªåŠ¨é€‰æ‹©æœ€ä½³ Tokenizerã€‚

# [DX Decision] ç”¨æˆ·åªéœ€è¦ä¼ å…¥æ¨¡åž‹åï¼ˆå¦‚ "gpt-4o"ï¼‰ï¼Œ
# å¼•æ“Žè‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„ Tokenizerã€‚è¿™æ˜¯"é›¶é…ç½®è¯†åˆ«"çš„æ ¸å¿ƒç»„ä»¶â€”â€”
# å¼€å‘è€…ä¸éœ€è¦çŸ¥é“ GPT-4o ç”¨çš„æ˜¯ o200k_base ç¼–ç æ–¹æ¡ˆã€‚

â†’ 6.2.1 ç‰©ç†çº¦æŸä¸Žæ€§èƒ½æ¨¡åž‹
"""

from __future__ import annotations

import logging

from context_forge.tokenizer.fallback import CharBasedCounter
from context_forge.tokenizer.protocol import TokenCounter
from context_forge.tokenizer.tiktoken_counter import TiktokenCounter

logger = logging.getLogger(__name__)

# æ¨¡åž‹åå‰ç¼€åˆ° tiktoken ç¼–ç æ–¹æ¡ˆçš„æ˜ å°„
# [Design Decision] ä½¿ç”¨å‰ç¼€åŒ¹é…è€Œéžç²¾ç¡®åŒ¹é…ï¼Œ
# å› ä¸ºæ¨¡åž‹åç»å¸¸åŒ…å«æ—¥æœŸåŽç¼€ï¼ˆå¦‚ claude-sonnet-4-5-20250514ï¼‰ï¼Œ
# å‰ç¼€åŒ¹é…æ›´å…·é²æ£’æ€§ã€‚
_MODEL_TO_ENCODING: dict[str, str] = {
    # OpenAI â€” GPT-4o ç³»åˆ—ä½¿ç”¨ o200k_base
    "gpt-4o": "o200k_base",
    "gpt-4o-mini": "o200k_base",
    "chatgpt-4o": "o200k_base",
    # OpenAI â€” GPT-4 / GPT-3.5 ä½¿ç”¨ cl100k_base
    "gpt-4-turbo": "cl100k_base",
    "gpt-4-": "cl100k_base",  # gpt-4-0125-preview ç­‰
    "gpt-4": "cl100k_base",
    "gpt-3.5": "cl100k_base",
    # OpenAI â€” Reasoning Models
    "o1": "o200k_base",
    "o3": "o200k_base",
    "o4-mini": "o200k_base",
    # Anthropic â€” ä½¿ç”¨ cl100k_base ä½œä¸ºè¿‘ä¼¼
    # ðŸ­ ç”Ÿäº§æç¤ºï¼šAnthropic æ²¡æœ‰å…¬å¼€çš„ Python tokenizerï¼Œ
    # ä½¿ç”¨ cl100k_base è¿‘ä¼¼è®¡æ•°ï¼Œè¯¯å·®é€šå¸¸åœ¨ 5% ä»¥å†…ã€‚
    # å¦‚éœ€ç²¾ç¡®è®¡æ•°ï¼Œå¯è°ƒç”¨ Anthropic çš„ count_tokens APIã€‚
    "claude": "cl100k_base",
    # Google â€” ä½¿ç”¨ cl100k_base ä½œä¸ºè¿‘ä¼¼
    # ðŸ­ ç”Ÿäº§æç¤ºï¼šGemini ä½¿ç”¨ SentencePiece tokenizerï¼Œ
    # å¦‚éœ€ç²¾ç¡®è®¡æ•°ï¼Œä½¿ç”¨ google-generativeai çš„ count_tokens() æ–¹æ³•ã€‚
    "gemini": "cl100k_base",
    # æœ¬åœ°æ¨¡åž‹ â€” ä½¿ç”¨ cl100k_base ä½œä¸ºè¿‘ä¼¼
    "llama": "cl100k_base",
    "mistral": "cl100k_base",
    "qwen": "cl100k_base",
    "deepseek": "cl100k_base",
}

# Tokenizer å®žä¾‹ç¼“å­˜ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
_counter_cache: dict[str, TokenCounter] = {}

# ç”¨æˆ·æ³¨å†Œçš„è‡ªå®šä¹‰ Tokenizer
_custom_counters: dict[str, TokenCounter] = {}


def get_tokenizer(model: str) -> TokenCounter:
    """
    æ ¹æ®æ¨¡åž‹åèŽ·å–æœ€ä½³ Token è®¡æ•°å™¨ã€‚

    æŸ¥æ‰¾ä¼˜å…ˆçº§ï¼š
    1. ç”¨æˆ·æ³¨å†Œçš„è‡ªå®šä¹‰ Tokenizer
    2. åŸºäºŽæ¨¡åž‹åå‰ç¼€åŒ¹é…çš„ tiktoken ç¼–ç æ–¹æ¡ˆ
    3. CharBasedCounter fallback

    å‚æ•°:
        model: æ¨¡åž‹åç§°ï¼ˆå¦‚ "gpt-4o"ã€"claude-sonnet-4-5-20250514"ï¼‰

    è¿”å›ž:
        TokenCounter å®žä¾‹

    ç¤ºä¾‹::

        counter = get_tokenizer("gpt-4o")
        tokens = counter.count("Hello, world!")
    """
    # 1. æ£€æŸ¥è‡ªå®šä¹‰æ³¨å†Œ
    if model in _custom_counters:
        return _custom_counters[model]

    # 2. æ£€æŸ¥ç¼“å­˜
    if model in _counter_cache:
        return _counter_cache[model]

    # 3. å‰ç¼€åŒ¹é…
    encoding_name = _find_encoding(model)

    if encoding_name:
        try:
            counter: TokenCounter = TiktokenCounter(encoding_name)
            _counter_cache[model] = counter
            return counter
        except Exception as e:
            logger.warning(
                "ä¸ºæ¨¡åž‹ '%s' åˆ›å»º tiktoken è®¡æ•°å™¨å¤±è´¥ï¼ˆç¼–ç ï¼š%sï¼‰ï¼Œ"
                "å›žé€€åˆ°å­—ç¬¦è®¡æ•°å™¨ã€‚é”™è¯¯ï¼š%s",
                model,
                encoding_name,
                e,
            )

    # 4. Fallback
    logger.info(
        "æ¨¡åž‹ '%s' æœªæ‰¾åˆ°ä¸“ç”¨ Tokenizerï¼Œä½¿ç”¨å­—ç¬¦è®¡æ•°å™¨ï¼ˆè¿‘ä¼¼å€¼ï¼‰ã€‚",
        model,
    )
    counter = CharBasedCounter()
    _counter_cache[model] = counter
    return counter


def _find_encoding(model: str) -> str | None:
    """é€šè¿‡å‰ç¼€åŒ¹é…æ‰¾åˆ°ç¼–ç æ–¹æ¡ˆã€‚"""
    model_lower = model.lower()

    # æŒ‰å‰ç¼€é•¿åº¦é™åºæŽ’åˆ—ï¼Œä¼˜å…ˆåŒ¹é…æ›´å…·ä½“çš„å‰ç¼€
    for prefix in sorted(_MODEL_TO_ENCODING.keys(), key=len, reverse=True):
        if model_lower.startswith(prefix):
            return _MODEL_TO_ENCODING[prefix]

    return None


def register_tokenizer(model: str, counter: TokenCounter) -> None:
    """
    æ³¨å†Œè‡ªå®šä¹‰ Token è®¡æ•°å™¨ã€‚

    æ³¨å†ŒåŽï¼Œè¯¥æ¨¡åž‹å°†ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰è®¡æ•°å™¨è€Œéžå†…ç½®çš„ tiktokenã€‚
    é€‚ç”¨äºŽéœ€è¦ç²¾ç¡®è®¡æ•°çš„åœºæ™¯ï¼ˆå¦‚ä½¿ç”¨ Anthropic API çš„ count_tokensï¼‰ã€‚

    å‚æ•°:
        model: æ¨¡åž‹åç§°
        counter: TokenCounter å®žä¾‹

    ç¤ºä¾‹::

        class AnthropicCounter:
            def count(self, text: str) -> int:
                # è°ƒç”¨ Anthropic API çš„ count_tokens
                return anthropic_client.count_tokens(text)

            def count_messages(self, messages: list[dict[str, str]]) -> int:
                return sum(self.count(m["content"]) for m in messages)

            @property
            def name(self) -> str:
                return "anthropic_api"

        register_tokenizer("claude-sonnet-4-5-20250514", AnthropicCounter())
    """
    if not isinstance(counter, TokenCounter):
        raise TypeError(
            f"counter å¿…é¡»å®žçŽ° TokenCounter åè®®ï¼Œ"
            f"ä½† {type(counter).__name__} ç¼ºå°‘å¿…è¦çš„æ–¹æ³•ã€‚"
            f"éœ€è¦å®žçŽ°ï¼šcount(text) -> int, count_messages(messages) -> int, name -> str"
        )
    _custom_counters[model] = counter
    logger.info("å·²ä¸ºæ¨¡åž‹ '%s' æ³¨å†Œè‡ªå®šä¹‰ Tokenizer: %s", model, counter.name)


def clear_cache() -> None:
    """æ¸…é™¤ Tokenizer ç¼“å­˜ã€‚é€šå¸¸ä»…åœ¨æµ‹è¯•ä¸­ä½¿ç”¨ã€‚"""
    _counter_cache.clear()
