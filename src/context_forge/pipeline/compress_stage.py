"""
å‹ç¼©é˜¶æ®µ â€” Pipeline ä¸­çš„å‹ç¼©æ­¥éª¤ã€‚

â†’ 6.1.2.1 Pipeline æ¶æ„ï¼š5 é˜¶æ®µæµæ°´çº¿

å‹ç¼©é˜¶æ®µåœ¨ Allocate ä¹‹åã€Assemble ä¹‹å‰æ‰§è¡Œï¼š
1. Normalizeï¼šå¡«å…… Token è®¡æ•°
2. Sanitizeï¼šæ¸…æ´—å’Œå®‰å…¨æ£€æŸ¥
3. Rerankï¼šæ’åºå’Œå»é‡
4. Allocateï¼šé¢„ç®—åˆ†é…
5. **Compress**ï¼šé¥±å’Œåº¦è§¦å‘çš„å‹ç¼©ï¼ˆæœ¬é˜¶æ®µï¼‰
6. Assembleï¼šæœ€ç»ˆç»„è£…

# [Design Decision] å‹ç¼©é˜¶æ®µæ”¾åœ¨ Allocate ä¹‹åï¼Œ
# å› ä¸ºåªæœ‰å®Œæˆé¢„ç®—åˆ†é…åæ‰èƒ½è®¡ç®—é¥±å’Œåº¦ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©ã€‚
"""

from __future__ import annotations

import logging
from typing import TYPE_CHECKING

from context_forge.compress.engine import CompressEngine
from context_forge.errors.exceptions import PipelineStageError
from context_forge.models.audit import AuditEntry, DecisionType, ReasonCode

if TYPE_CHECKING:
    from context_forge.models.segment import Segment
    from context_forge.pipeline.base import PipelineContext

logger = logging.getLogger(__name__)


class CompressStage:
    """
    å‹ç¼©é˜¶æ®µ â€” é¥±å’Œåº¦è§¦å‘çš„è‡ªé€‚åº”å‹ç¼©ã€‚

    â†’ 6.2.4.3 å‹ç¼©é˜¶æ®µé›†æˆ

    å‹ç¼©é˜¶æ®µä½¿ç”¨ CompressEngine æ‰§è¡Œå‹ç¼©ï¼š
    - ä» PipelineContext è¯»å–é¢„ç®—ä¿¡æ¯ï¼ˆavailable_tokensï¼‰
    - è®¡ç®—é¥±å’Œåº¦å¹¶è§¦å‘å‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰
    - å°†å‹ç¼©å†³ç­–è®°å½•åˆ°å®¡è®¡æ—¥å¿—

    åŸºæœ¬ç”¨æ³•ï¼ˆåœ¨ Pipeline ä¸­è‡ªåŠ¨è°ƒç”¨ï¼‰::

        stage = CompressStage(engine=CompressEngine())
        compressed = await stage.process(segments, context)

    è‡ªå®šä¹‰å‹ç¼©å¼•æ“::

        from context_forge.compress.engine import CompressEngine
        from context_forge.compress.summary import LLMSummaryCompressor

        engine = CompressEngine(
            saturation_threshold=0.9,  # æ›´é«˜çš„é˜ˆå€¼
            default_compressor=LLMSummaryCompressor(provider),
        )
        stage = CompressStage(engine=engine)

    å±æ€§:
        engine: å‹ç¼©å¼•æ“ï¼ˆCompressEngine å®ä¾‹ï¼‰
    """

    def __init__(self, engine: CompressEngine | None = None):
        """
        åˆå§‹åŒ–å‹ç¼©é˜¶æ®µã€‚

        å‚æ•°:
            engine: å‹ç¼©å¼•æ“ï¼ŒNone æ—¶ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self._engine = engine or CompressEngine()

    @property
    def name(self) -> str:
        """é˜¶æ®µåç§°ã€‚"""
        return "compress"

    async def process(
        self, segments: list[Segment], context: PipelineContext
    ) -> list[Segment]:
        """
        æ‰§è¡Œå‹ç¼©é˜¶æ®µã€‚

        â†’ 6.2.4.3 å‹ç¼©è§¦å‘é€»è¾‘

        æµç¨‹:
        1. ä» PipelineContext è¯»å–é¢„ç®—ä¿¡æ¯
        2. è°ƒç”¨ CompressEngine æ‰§è¡Œå‹ç¼©
        3. å°†å‹ç¼©å†³ç­–è®°å½•åˆ°å®¡è®¡æ—¥å¿—
        4. è¿”å›å‹ç¼©åçš„ Segment åˆ—è¡¨

        å‚æ•°:
            segments: è¾“å…¥ Segment åˆ—è¡¨ï¼ˆå·²å®Œæˆ Allocateï¼‰
            context: Pipeline ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«é¢„ç®—ä¿¡æ¯ï¼‰

        è¿”å›:
            å‹ç¼©åçš„ Segment åˆ—è¡¨

        æŠ›å‡º:
            PipelineStageError: å‹ç¼©å¤±è´¥
        """
        if not segments:
            return []

        # ä» context è¯»å–é¢„ç®—ä¿¡æ¯
        # ğŸ­ ç”Ÿäº§æç¤ºï¼šPipelineContext åº”è¯¥åŒ…å« budget_info å­—æ®µ
        # è¿™é‡Œå‡è®¾é€šè¿‡ metadata ä¼ é€’
        available_tokens = context.metadata.get("available_tokens", 100_000)
        model_name = context.metadata.get("model_name")

        # è®°å½•å‹ç¼©å‰çŠ¶æ€
        original_count = len(segments)
        original_tokens = sum(seg.token_count or 0 for seg in segments)

        logger.info(
            f"å‹ç¼©é˜¶æ®µå¼€å§‹ï¼š{original_count} ä¸ª Segmentï¼Œ{original_tokens} Token"
        )

        try:
            # è°ƒç”¨å‹ç¼©å¼•æ“
            compressed_segments = await self._engine.compress(
                segments=segments,
                available_tokens=available_tokens,
                audit_log=context.audit_log,
                model_name=model_name,
            )
        except Exception as e:
            logger.error(f"å‹ç¼©é˜¶æ®µå¤±è´¥ï¼š{e}")
            raise PipelineStageError(
                stage_name=self.name,
                what="å‹ç¼©å¤±è´¥",
                why=str(e),
                how="æ£€æŸ¥å‹ç¼©å¼•æ“é…ç½®æˆ–å¢åŠ é¢„ç®—",
            ) from e

        # è®°å½•å‹ç¼©åçŠ¶æ€
        compressed_count = len(compressed_segments)
        compressed_tokens = sum(seg.token_count or 0 for seg in compressed_segments)

        logger.info(
            f"å‹ç¼©é˜¶æ®µå®Œæˆï¼š{compressed_count} ä¸ª Segmentï¼Œ{compressed_tokens} Token "
            f"(èŠ‚çœ {original_tokens - compressed_tokens} Token)"
        )

        # è®°å½•å®¡è®¡æ—¥å¿—
        if original_tokens != compressed_tokens:
            # ä¸ºå‹ç¼©æ“ä½œåˆ›å»ºå•æ¡å®¡è®¡è®°å½•ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ª segment çš„ ID ä½œä¸ºä»£è¡¨ï¼‰
            # ğŸ­ ç”Ÿäº§æç¤ºï¼šå¯ä»¥ä¸ºæ¯ä¸ªè¢«å‹ç¼©çš„ Segment åˆ›å»ºå•ç‹¬çš„å®¡è®¡è®°å½•
            representative_id = segments[0].id if segments else "batch_compress"
            context.audit_log.append(
                AuditEntry(
                    segment_id=representative_id,
                    decision=DecisionType.COMPRESS,
                    reason_code=ReasonCode.COMPRESS_WINDOW_SATURATION,
                    reason_detail=(
                        f"é¥±å’Œåº¦è¶…è¿‡é˜ˆå€¼ï¼Œå‹ç¼© {original_count - compressed_count} ä¸ª Segmentï¼Œ"
                        f"èŠ‚çœ {original_tokens - compressed_tokens} Token"
                    ),
                    pipeline_stage=self.name,
                    token_impact=-(original_tokens - compressed_tokens),
                    metadata={
                        "original_count": original_count,
                        "compressed_count": compressed_count,
                        "original_tokens": original_tokens,
                        "compressed_tokens": compressed_tokens,
                        "compression_ratio": compressed_tokens / original_tokens
                        if original_tokens > 0
                        else 1.0,
                        "affected_segment_ids": [seg.id for seg in segments],
                    },
                )
            )

        return compressed_segments
