"""
é»˜è®¤é…ç½®ä¸Žæ¨¡åž‹æ³¨å†Œè¡¨ã€‚

# [DX Decision] å†…ç½®å¸¸è§æ¨¡åž‹çš„å®Œæ•´é…ç½®ä¿¡æ¯ï¼Œ
# è®©ç”¨æˆ·ä¼ å…¥æ¨¡åž‹åå³å¯è‡ªåŠ¨åŒ¹é…çª—å£å¤§å°ã€tokenizerã€æˆæœ¬ç­‰ï¼Œ
# æ— éœ€æ‰‹åŠ¨æŸ¥é˜…å„åŽ‚å•†æ–‡æ¡£ã€‚è¿™æ˜¯"é›¶é…ç½®è¯†åˆ«"çš„æ•°æ®åŸºç¡€ã€‚

â†’ 6.6.1 æ„å›¾é©±åŠ¨è·¯ç”±
"""

from __future__ import annotations

from context_forge.models.routing import ModelConfig

# ============================================================
# æ¨¡åž‹æ³¨å†Œè¡¨
# åŒ…å«ä¸»æµ LLM çš„é…ç½®ä¿¡æ¯ã€‚æ•°æ®æ¥æºä¸ºå„åŽ‚å•†å®˜æ–¹æ–‡æ¡£ï¼ˆæˆªæ­¢ 2025 å¹´ï¼‰ã€‚
#
# ðŸ­ ç”Ÿäº§æç¤ºï¼šæ¨¡åž‹ä¿¡æ¯ä¼šéšæ—¶é—´æ›´æ–°ï¼ˆæ–°æ¨¡åž‹å‘å¸ƒã€ä»·æ ¼è°ƒæ•´ï¼‰ã€‚
# å»ºè®®é€šè¿‡ YAML ç­–ç•¥æ–‡ä»¶è¦†ç›–é»˜è®¤å€¼ï¼Œæˆ–å®šæœŸæ›´æ–°æ­¤æ³¨å†Œè¡¨ã€‚
# ============================================================

MODEL_REGISTRY: dict[str, ModelConfig] = {
    # --- OpenAI ---
    "gpt-4o": ModelConfig(
        model_id="gpt-4o",
        provider="openai",
        max_context_tokens=128_000,
        max_output_tokens=16_384,
        tokenizer_name="o200k_base",
        supports_thinking=False,
        supports_tool_use=True,
        supports_vision=True,
        cost_per_million_input=2.50,
        cost_per_million_output=10.00,
    ),
    "gpt-4o-mini": ModelConfig(
        model_id="gpt-4o-mini",
        provider="openai",
        max_context_tokens=128_000,
        max_output_tokens=16_384,
        tokenizer_name="o200k_base",
        supports_thinking=False,
        supports_tool_use=True,
        supports_vision=True,
        cost_per_million_input=0.15,
        cost_per_million_output=0.60,
    ),
    "gpt-4-turbo": ModelConfig(
        model_id="gpt-4-turbo",
        provider="openai",
        max_context_tokens=128_000,
        max_output_tokens=4_096,
        tokenizer_name="cl100k_base",
        supports_thinking=False,
        supports_tool_use=True,
        supports_vision=True,
        cost_per_million_input=10.00,
        cost_per_million_output=30.00,
    ),
    "o1": ModelConfig(
        model_id="o1",
        provider="openai",
        max_context_tokens=200_000,
        max_output_tokens=100_000,
        tokenizer_name="o200k_base",
        supports_thinking=True,
        supports_tool_use=True,
        supports_vision=True,
        cost_per_million_input=15.00,
        cost_per_million_output=60.00,
    ),
    "o3-mini": ModelConfig(
        model_id="o3-mini",
        provider="openai",
        max_context_tokens=200_000,
        max_output_tokens=100_000,
        tokenizer_name="o200k_base",
        supports_thinking=True,
        supports_tool_use=True,
        supports_vision=False,
        cost_per_million_input=1.10,
        cost_per_million_output=4.40,
    ),
    # --- Anthropic ---
    "claude-opus-4-20250115": ModelConfig(
        model_id="claude-opus-4-20250115",
        provider="anthropic",
        max_context_tokens=200_000,
        max_output_tokens=32_000,
        tokenizer_name="cl100k_base",  # è¿‘ä¼¼
        supports_thinking=True,
        supports_tool_use=True,
        supports_vision=True,
        cost_per_million_input=15.00,
        cost_per_million_output=75.00,
    ),
    "claude-sonnet-4-5-20250514": ModelConfig(
        model_id="claude-sonnet-4-5-20250514",
        provider="anthropic",
        max_context_tokens=200_000,
        max_output_tokens=16_000,
        tokenizer_name="cl100k_base",  # è¿‘ä¼¼
        supports_thinking=True,
        supports_tool_use=True,
        supports_vision=True,
        cost_per_million_input=3.00,
        cost_per_million_output=15.00,
    ),
    "claude-haiku-3-5-20241022": ModelConfig(
        model_id="claude-haiku-3-5-20241022",
        provider="anthropic",
        max_context_tokens=200_000,
        max_output_tokens=8_192,
        tokenizer_name="cl100k_base",  # è¿‘ä¼¼
        supports_thinking=False,
        supports_tool_use=True,
        supports_vision=True,
        cost_per_million_input=0.80,
        cost_per_million_output=4.00,
    ),
    # --- Google ---
    "gemini-2.0-flash": ModelConfig(
        model_id="gemini-2.0-flash",
        provider="google",
        max_context_tokens=1_048_576,
        max_output_tokens=8_192,
        tokenizer_name="cl100k_base",  # è¿‘ä¼¼
        supports_thinking=True,
        supports_tool_use=True,
        supports_vision=True,
        cost_per_million_input=0.10,
        cost_per_million_output=0.40,
    ),
    "gemini-2.5-pro": ModelConfig(
        model_id="gemini-2.5-pro",
        provider="google",
        max_context_tokens=1_048_576,
        max_output_tokens=65_536,
        tokenizer_name="cl100k_base",  # è¿‘ä¼¼
        supports_thinking=True,
        supports_tool_use=True,
        supports_vision=True,
        cost_per_million_input=1.25,
        cost_per_million_output=10.00,
    ),
    # --- æœ¬åœ°/å¼€æºæ¨¡åž‹ ---
    "llama-3.1-70b": ModelConfig(
        model_id="llama-3.1-70b",
        provider="local",
        max_context_tokens=128_000,
        max_output_tokens=4_096,
        tokenizer_name="cl100k_base",  # è¿‘ä¼¼
        supports_thinking=False,
        supports_tool_use=True,
        supports_vision=False,
        cost_per_million_input=0.0,
        cost_per_million_output=0.0,
    ),
    "deepseek-v3": ModelConfig(
        model_id="deepseek-v3",
        provider="deepseek",
        max_context_tokens=128_000,
        max_output_tokens=8_192,
        tokenizer_name="cl100k_base",  # è¿‘ä¼¼
        supports_thinking=False,
        supports_tool_use=True,
        supports_vision=False,
        cost_per_million_input=0.27,
        cost_per_million_output=1.10,
    ),
    "qwen-2.5-72b": ModelConfig(
        model_id="qwen-2.5-72b",
        provider="local",
        max_context_tokens=131_072,
        max_output_tokens=8_192,
        tokenizer_name="cl100k_base",  # è¿‘ä¼¼
        supports_thinking=False,
        supports_tool_use=True,
        supports_vision=False,
        cost_per_million_input=0.0,
        cost_per_million_output=0.0,
    ),
}

# å¸¸ç”¨æ¨¡åž‹åˆ«åæ˜ å°„
# [DX Decision] è®©ç”¨æˆ·å¯ä»¥ç”¨ç®€çŸ­åˆ«åè€Œéžå®Œæ•´æ¨¡åž‹ ID
MODEL_ALIASES: dict[str, str] = {
    "gpt4o": "gpt-4o",
    "gpt4o-mini": "gpt-4o-mini",
    "4o": "gpt-4o",
    "4o-mini": "gpt-4o-mini",
    "claude-opus": "claude-opus-4-20250115",
    "claude-sonnet": "claude-sonnet-4-5-20250514",
    "claude-haiku": "claude-haiku-3-5-20241022",
    "opus": "claude-opus-4-20250115",
    "sonnet": "claude-sonnet-4-5-20250514",
    "haiku": "claude-haiku-3-5-20241022",
    "gemini-flash": "gemini-2.0-flash",
    "gemini-pro": "gemini-2.5-pro",
    "llama": "llama-3.1-70b",
    "deepseek": "deepseek-v3",
    "qwen": "qwen-2.5-72b",
}


def resolve_model(model_id: str) -> ModelConfig:
    """
    è§£æžæ¨¡åž‹åç§°å¹¶è¿”å›žå®Œæ•´çš„æ¨¡åž‹é…ç½®ã€‚

    æ”¯æŒï¼š
    1. ç²¾ç¡®åŒ¹é…æ¨¡åž‹ ID
    2. åˆ«ååŒ¹é…
    3. å‰ç¼€åŒ¹é…ï¼ˆå¦‚ "claude-sonnet" åŒ¹é… "claude-sonnet-4-5-20250514"ï¼‰

    å‚æ•°:
        model_id: æ¨¡åž‹åç§°æˆ–åˆ«å

    è¿”å›ž:
        ModelConfig å®žä¾‹

    å¼‚å¸¸:
        ModelNotFoundError: æœªæ‰¾åˆ°åŒ¹é…çš„æ¨¡åž‹
    """
    from context_forge.errors import ModelNotFoundError

    # 1. ç²¾ç¡®åŒ¹é…
    if model_id in MODEL_REGISTRY:
        return MODEL_REGISTRY[model_id]

    # 2. åˆ«ååŒ¹é…
    resolved = MODEL_ALIASES.get(model_id)
    if resolved and resolved in MODEL_REGISTRY:
        return MODEL_REGISTRY[resolved]

    # 3. å‰ç¼€åŒ¹é…ï¼ˆä»Žæœ€é•¿å‰ç¼€å¼€å§‹ï¼‰
    model_lower = model_id.lower()
    for registry_id in sorted(MODEL_REGISTRY.keys(), key=len, reverse=True):
        if model_lower.startswith(registry_id.lower()):
            return MODEL_REGISTRY[registry_id]
        if registry_id.lower().startswith(model_lower):
            return MODEL_REGISTRY[registry_id]

    available = list(MODEL_REGISTRY.keys()) + list(MODEL_ALIASES.keys())
    raise ModelNotFoundError(
        what=f"æœªæ‰¾åˆ°æ¨¡åž‹ '{model_id}'ã€‚",
        why="è¯¥æ¨¡åž‹ä¸åœ¨å†…ç½®æ¨¡åž‹æ³¨å†Œè¡¨ä¸­ï¼Œä¹Ÿæœªé€šè¿‡è‡ªå®šä¹‰é…ç½®æ³¨å†Œã€‚",
        how=f"æ£€æŸ¥æ¨¡åž‹åç§°æ˜¯å¦æ­£ç¡®ã€‚å¯ç”¨çš„æ¨¡åž‹å’Œåˆ«åï¼š{', '.join(sorted(available)[:10])} ç­‰ã€‚"
            f"å¦‚éœ€æ·»åŠ è‡ªå®šä¹‰æ¨¡åž‹ï¼Œä½¿ç”¨ register_model() æ–¹æ³•ã€‚",
        model_id=model_id,
        available_models=sorted(available),
    )


def register_model(model_id: str, config: ModelConfig) -> None:
    """
    æ³¨å†Œè‡ªå®šä¹‰æ¨¡åž‹é…ç½®ã€‚

    å‚æ•°:
        model_id: æ¨¡åž‹ ID
        config: æ¨¡åž‹é…ç½®

    ç¤ºä¾‹::

        register_model("my-local-model", ModelConfig(
            model_id="my-local-model",
            provider="local",
            max_context_tokens=32768,
            max_output_tokens=4096,
        ))
    """
    MODEL_REGISTRY[model_id] = config


def list_models() -> list[str]:
    """è¿”å›žæ‰€æœ‰å·²æ³¨å†Œæ¨¡åž‹çš„ ID åˆ—è¡¨ã€‚"""
    return sorted(MODEL_REGISTRY.keys())
