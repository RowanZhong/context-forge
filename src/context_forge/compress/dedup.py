"""
å»é‡å‹ç¼©å™¨ â€” åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„å»é‡ã€‚

â†’ 6.2.4.2 åŸºäºè§„åˆ™çš„å‹ç¼©ç­–ç•¥ï¼šå»é‡

åœ¨ RAG åœºæ™¯ä¸­ï¼Œæ£€ç´¢ç»“æœå¸¸æœ‰é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„ç‰‡æ®µã€‚
å»é‡å‹ç¼©å™¨ä½¿ç”¨ n-gram Jaccard ç›¸ä¼¼åº¦æ£€æµ‹é‡å¤ï¼Œä¿ç•™é«˜ä¼˜å…ˆçº§æˆ–é«˜åˆ†æ•°çš„ç‰ˆæœ¬ã€‚

# [Design Decision] ä½¿ç”¨ n-gram Jaccard è€ŒéåµŒå…¥å‘é‡ï¼Œ
# å› ä¸ºé›¶ä¾èµ–ã€é›¶å»¶è¿Ÿï¼Œä¸”å¯¹æ–‡æœ¬é‡å¤æ£€æµ‹æ•ˆæœè¶³å¤Ÿå¥½ï¼ˆF1 > 0.9ï¼‰ã€‚
# å¦‚éœ€æ›´ç²¾ç¡®çš„è¯­ä¹‰å»é‡ï¼Œå¯ä»¥ç»§æ‰¿æœ¬ç±»å¹¶é‡å†™ _compute_similarity æ–¹æ³•ã€‚
"""

from __future__ import annotations

from typing import TYPE_CHECKING

from context_forge.compress.base import CompressContext, CompressionResult
from context_forge.models.provenance import Provenance, SourceType

if TYPE_CHECKING:
    from context_forge.models.segment import Segment


class DedupCompressor:
    """
    å»é‡å‹ç¼©å™¨ â€” åˆ é™¤é‡å¤æˆ–é«˜åº¦ç›¸ä¼¼çš„ Segmentã€‚

    â†’ 6.2.4.2 å»é‡ç­–ç•¥

    å»é‡æµç¨‹:
    1. è®¡ç®—æ‰€æœ‰ Segment ä¸¤ä¸¤ä¹‹é—´çš„ Jaccard ç›¸ä¼¼åº¦
    2. ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„ Segment å¯¹è§†ä¸ºé‡å¤
    3. ä¿ç•™ä¼˜å…ˆçº§æ›´é«˜æˆ–æ£€ç´¢åˆ†æ•°æ›´é«˜çš„ç‰ˆæœ¬
    4. åˆ é™¤é‡å¤é¡¹

    åŸºæœ¬ç”¨æ³•::

        compressor = DedupCompressor(similarity_threshold=0.85)
        result = await compressor.compress(segments, context)

    è°ƒæ•´ n-gram å¤§å°::

        compressor = DedupCompressor(
            similarity_threshold=0.85,
            ngram_size=3,  # ä½¿ç”¨ 3-gramï¼ˆé»˜è®¤ 2-gramï¼‰
        )

    å±æ€§:
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è§†ä¸ºé‡å¤ï¼Œå–å€¼èŒƒå›´ [0, 1]
        ngram_size: n-gram å¤§å°ï¼Œé»˜è®¤ 2ï¼ˆbigramï¼‰
    """

    def __init__(self, similarity_threshold: float = 0.85, ngram_size: int = 2):
        """
        åˆå§‹åŒ–å»é‡å‹ç¼©å™¨ã€‚

        å‚æ•°:
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ 0.85ï¼‰
            ngram_size: n-gram å¤§å°ï¼ˆé»˜è®¤ 2ï¼‰
        """
        self._threshold = max(0.0, min(1.0, similarity_threshold))
        self._ngram_size = max(1, ngram_size)

    @property
    def name(self) -> str:
        """å‹ç¼©å™¨åç§°ã€‚"""
        return f"dedup_jaccard_{self._ngram_size}gram"

    async def compress(
        self, segments: list[Segment], context: CompressContext
    ) -> CompressionResult:
        """
        å»é‡å‹ç¼© Segment åˆ—è¡¨ã€‚

        â†’ 6.2.4.2 å»é‡ç®—æ³•å®ç°

        æµç¨‹:
        1. è®¡ç®—æ‰€æœ‰ Segment çš„ n-gram é›†åˆ
        2. éå† Segment å¯¹ï¼Œè®¡ç®— Jaccard ç›¸ä¼¼åº¦
        3. æ ‡è®°ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„ Segment ä¸ºé‡å¤
        4. æ ¹æ®ä¼˜å…ˆçº§å’Œæ£€ç´¢åˆ†æ•°é€‰æ‹©ä¿ç•™ç‰ˆæœ¬
        5. æ›´æ–° Provenance

        å‚æ•°:
            segments: å¾…å»é‡çš„ Segment åˆ—è¡¨
            context: å‹ç¼©ä¸Šä¸‹æ–‡

        è¿”å›:
            CompressionResultï¼ŒåŒ…å«å»é‡åçš„ Segment
        """
        if not segments:
            return CompressionResult(
                compressed_segments=[],
                original_token_count=0,
                compressed_token_count=0,
                method=self.name,
                parent_segment_ids=[],
            )

        # è®¡ç®—åŸå§‹æ€» Token æ•°
        original_tokens = sum(seg.token_count or 0 for seg in segments)

        # è®¡ç®—æ‰€æœ‰ Segment çš„ n-gram é›†åˆ
        ngram_sets = [self._compute_ngrams(seg.content) for seg in segments]

        # æ ‡è®°è¦ä¿ç•™çš„ Segmentï¼ˆåˆå§‹å…¨éƒ¨ä¿ç•™ï¼‰
        to_keep = [True] * len(segments)

        # éå†æ‰€æœ‰ Segment å¯¹ï¼Œæ ‡è®°é‡å¤é¡¹
        for i in range(len(segments)):
            if not to_keep[i]:
                continue  # å·²è¢«æ ‡è®°ä¸ºåˆ é™¤

            for j in range(i + 1, len(segments)):
                if not to_keep[j]:
                    continue

                # è®¡ç®— Jaccard ç›¸ä¼¼åº¦
                similarity = self._jaccard_similarity(ngram_sets[i], ngram_sets[j])

                if similarity >= self._threshold:
                    # é‡å¤ï¼é€‰æ‹©ä¿ç•™å“ªä¸ª
                    # ä¼˜å…ˆçº§ï¼šCRITICAL > HIGH > MEDIUM > LOW
                    # ç›¸åŒä¼˜å…ˆçº§æ—¶æ¯”è¾ƒæ£€ç´¢åˆ†æ•°ï¼ˆRAG åœºæ™¯ï¼‰
                    if self._should_keep_first(segments[i], segments[j]):
                        to_keep[j] = False
                    else:
                        to_keep[i] = False
                        break  # å½“å‰ Segment å·²è¢«åˆ é™¤ï¼Œè·³å‡ºå†…å±‚å¾ªç¯

        # æ”¶é›†ä¿ç•™çš„ Segment
        kept_segments = [seg for idx, seg in enumerate(segments) if to_keep[idx]]

        # æ›´æ–° Provenanceï¼ˆæ ‡è®°ä¸ºå»é‡æ¥æºï¼‰
        parent_ids = [seg.id for seg in segments]
        updated_segments = []
        for seg in kept_segments:
            new_provenance = Provenance(
                source_id=f"dedup_{seg.id}",
                source_type=SourceType.COMPRESSION,
                parent_segment_ids=parent_ids,
                compression_method=self.name,
            )
            updated_segments.append(
                seg.model_copy(update={"provenance": new_provenance})
            )

        compressed_tokens = sum(seg.token_count or 0 for seg in updated_segments)

        # ğŸ­ ç”Ÿäº§æç¤ºï¼šè®°å½•è¢«åˆ é™¤çš„ Segment ID ç”¨äºè°ƒè¯•
        removed_count = len(segments) - len(kept_segments)
        metadata = {"removed_count": removed_count, "threshold": self._threshold}

        return CompressionResult(
            compressed_segments=updated_segments,
            original_token_count=original_tokens,
            compressed_token_count=compressed_tokens,
            method=self.name,
            parent_segment_ids=parent_ids,
            metadata=metadata,
        )

    def _compute_ngrams(self, text: str) -> set[str]:
        """
        è®¡ç®—æ–‡æœ¬çš„ n-gram é›†åˆã€‚

        å‚æ•°:
            text: è¾“å…¥æ–‡æœ¬

        è¿”å›:
            n-gram å­—ç¬¦ä¸²é›†åˆ
        """
        # ç®€å•åˆ†è¯ï¼ˆæŒ‰ç©ºç™½ç¬¦ï¼‰
        # ğŸ­ ç”Ÿäº§æç¤ºï¼šå¯¹ä¸­æ–‡åº”è¯¥ä½¿ç”¨å­—ç¬¦çº§ n-gram æˆ–ä¸“ç”¨åˆ†è¯å™¨
        tokens = text.split()
        ngrams = set()

        for i in range(len(tokens) - self._ngram_size + 1):
            ngram = " ".join(tokens[i : i + self._ngram_size])
            ngrams.add(ngram)

        return ngrams

    def _jaccard_similarity(self, set1: set[str], set2: set[str]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªé›†åˆçš„ Jaccard ç›¸ä¼¼åº¦ã€‚

        Jaccard(A, B) = |A âˆ© B| / |A âˆª B|

        å‚æ•°:
            set1: é›†åˆ 1
            set2: é›†åˆ 2

        è¿”å›:
            ç›¸ä¼¼åº¦ï¼Œå–å€¼èŒƒå›´ [0, 1]
        """
        if not set1 and not set2:
            return 1.0  # ä¸¤ä¸ªç©ºé›†å®Œå…¨ç›¸åŒ

        if not set1 or not set2:
            return 0.0  # ä¸€ä¸ªç©ºä¸€ä¸ªéç©ºï¼Œå®Œå…¨ä¸åŒ

        intersection = len(set1 & set2)
        union = len(set1 | set2)

        return intersection / union if union > 0 else 0.0

    def _should_keep_first(self, seg1: Segment, seg2: Segment) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿ç•™ç¬¬ä¸€ä¸ª Segmentï¼ˆåˆ é™¤ç¬¬äºŒä¸ªï¼‰ã€‚

        å†³ç­–ä¼˜å…ˆçº§:
        1. ä¼˜å…ˆçº§é«˜çš„ä¿ç•™
        2. ä¼˜å…ˆçº§ç›¸åŒæ—¶ï¼Œæ£€ç´¢åˆ†æ•°é«˜çš„ä¿ç•™ï¼ˆRAG åœºæ™¯ï¼‰
        3. éƒ½ç›¸åŒæ—¶ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆç¨³å®šæ’åºï¼‰

        å‚æ•°:
            seg1: Segment 1
            seg2: Segment 2

        è¿”å›:
            True ä¿ç•™ seg1ï¼ŒFalse ä¿ç•™ seg2
        """
        # æ¯”è¾ƒä¼˜å…ˆçº§
        # Priority æšä¸¾é¡ºåºï¼šCRITICAL > HIGH > MEDIUM > LOW
        priority_order = {"critical": 4, "high": 3, "medium": 2, "low": 1}
        p1 = priority_order.get(seg1.effective_priority.value, 0)
        p2 = priority_order.get(seg2.effective_priority.value, 0)

        if p1 != p2:
            return p1 > p2

        # ä¼˜å…ˆçº§ç›¸åŒï¼Œæ¯”è¾ƒæ£€ç´¢åˆ†æ•°
        score1 = (
            seg1.provenance.retrieval_score if seg1.provenance else 0.0
        ) or 0.0
        score2 = (
            seg2.provenance.retrieval_score if seg2.provenance else 0.0
        ) or 0.0

        if score1 != score2:
            return score1 > score2

        # éƒ½ç›¸åŒï¼Œä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆç¨³å®šæ’åºï¼‰
        return True
